# DevOps AI Assistant с бесплатной моделью ИИ

В этом документе описаны шаги для настройки и запуска DevOps AI Assistant с бесплатной локальной моделью искусственного интеллекта.

## Что вы получите

- Полностью бесплатную модель ИИ для ответов на вопросы по DevOps
- Работа без подключения к интернету (после установки модели)
- Отсутствие ограничений на количество запросов
- Полная конфиденциальность (ваши данные никуда не отправляются)

## Доступные модели

1. **llama3** - Модель от Meta, отличный баланс качества и размера
2. **orca-mini** - Быстрая и компактная модель с хорошим качеством ответов
3. **phi3** - Компактная и мощная модель от Microsoft

## Системные требования

- **Процессор:** 4+ ядра, x86-64
- **Оперативная память:** минимум 8 ГБ, рекомендуется 16 ГБ
- **Место на диске:** 4-8 ГБ в зависимости от выбранной модели
- **ОС:** Windows 10/11, macOS, Linux

## Установка

### Автоматическая установка

1. **Windows**: Запустите файл `setup_free_ai.bat`
2. **Linux/macOS**: Запустите файл `setup_free_ai.sh`

```bash
# Linux/macOS
chmod +x setup_free_ai.sh
./setup_free_ai.sh
```

### Ручная установка

1. **Установка Ollama**:
   - Скачайте и установите Ollama с [официального сайта](https://ollama.com/download)

2. **Запуск Ollama**:
   ```bash
   ollama serve
   ```

3. **Загрузка модели** (в отдельном терминале):
   ```bash
   ollama pull llama3
   ```

4. **Настройка проекта**:
   - Создайте файл `.env` в каталоге `backAnd` со следующим содержимым:
   ```
   DEFAULT_AI_PROVIDER=ollama
   OLLAMA_API_URL=http://localhost:11434
   OLLAMA_USE_FALLBACK=True
   ```

## Запуск проекта

После установки модели, запустите сервер Django:

```bash
# Windows
python manage.py runserver

# Linux/macOS
python3 manage.py runserver
```

Откройте веб-браузер и перейдите по адресу: http://localhost:8000

## Использование

1. В интерфейсе выберите вкладку "Ollama"
2. Выберите установленную модель из списка (например, llama3)
3. Задавайте вопросы по тематике DevOps, Unix-систем, сетей, контейнеризации и т.д.

## FAQ

### Почему модель отвечает медленно?

Скорость ответа зависит от характеристик вашего компьютера. Более быстрые ответы можно получить:
- Используя более компактную модель (например, orca-mini вместо llama3)
- Запуская на компьютере с более мощным процессором
- На компьютерах с GPU ответы будут генерироваться заметно быстрее

### Модель дает неправильные ответы

Локальные модели имеют ограничения по сравнению с большими коммерческими моделями. Если вы получаете неточные ответы:
- Попробуйте переформулировать вопрос
- Уточните детали в вопросе
- Используйте режим AI Team, чтобы задействовать несколько моделей одновременно

### Могу ли я использовать другие модели?

Да, с помощью Ollama вы можете установить и другие модели:

```bash
ollama pull mistral
ollama pull codellama
```

После установки они автоматически появятся в списке доступных моделей в интерфейсе. 